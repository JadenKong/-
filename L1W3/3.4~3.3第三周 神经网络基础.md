
# 3.4~3.5第三周 神经网络基础
[TOC]

### 3.4 多样本向量化（Vectorizing across multiple examples）

* ##### 多样本的神经网络
	* 逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的。
	* $x^{(1)}---->a^{[2](1)}=\hat{y}^{(1)}$
	* $x^{(2)}---->a^{[2](2)}=\hat{y}^{(2)}$
	* ...
	* $x^{(m)}---->a^{[2](m)}=\hat{y}^{(m)}$

* ##### 非向量化的形式实现多样本
	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$

* ##### 向量化的形式实现多样本
	* 多样本的第0层（输入层）
		 * 定义矩阵X 等于训练样本，将它们组合成矩阵的各列，形成一个（nx,m）维矩阵，其中nx表示单个样本的特征向量数量，m表示训练样本数量。
		 * 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有特征向量。
	
	* 多样本的第2层（输出层）
		* $W^{[2]}$ 表示[$w_1^{[2]}$]的矩阵，其中每个$w_i^{[2]}$长度对应隐藏层有多少个神经元输出结果，所以$W^{[2]}$是一个（输出层神经元数，隐藏层输出结果数）矩阵
		* $Z^{[2]}$表示 [$z^{[2](1)}$...,$z^{[2](m)}$]
		* $A^{[2]}$表示 [$a^{[2](1)}$...,$a^{[2](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有输出层神经单元。
<br>

### 3.5 向量化实现的解释（Justification for vectorized implementation）
* #####  以$Z^{[1]}=W^{[1]}X+b^{[1]}$为例子解释向量化
	* 我们先手动对几个样本计算一下前向传播，看有什么规律：
	* $z^{[1](1)}=W^{[1]}x^{(1)}+b^{[1]}$
	* $z^{[1](2)}=W^{[1]}x^{(2)}+b^{[1]}$
	* $W^{[1]}$是一个（4，nx）矩阵，4是由隐藏层有多少个神经单元决定，每个样本$x^{(1)},x^{(2)}$都是列向量，根据矩阵乘法，矩阵乘以列向量得到列向量:
	* ![730faaf8.png](:storage\a7715ab7-7d65-4d2b-8e04-9333d8850de0\730faaf8.png)
	* 这些列向量就组成了$W^{[1]}$点乘X=$Z^{[1]}$矩阵
	* $Z^{[1]}$表示 [$z^{[1](1)}$...,$z^{[1](m)}$]，每个$z^{[1](i)}$都是一个列向量
	* 最后往矩阵每个列向量加实数$b^{(i)}$，利用Python广播原理，$b^{(i)}$会加到每个列向量的每一个元素里,而$b^{(i)}$也组成了矩阵$b^{[1]}$。

<br>

* ##### 推广到其他公式
	* 上面我们论证了把for循环里，$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$公式向量化为$Z^{[1]}=W^{[1]}X+b^{[1]}$，同理可推出其余公式的向量化：
	* 	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$
	* 向量化：
		* $Z^{[1]}=W^{[1]}X+b^{[1]}$
		* $A^{[1]}=\sigma(Z^{[1]})$
		* $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
		* $A^{[2]}=\sigma(Z^{[2]})$
		* 其中X也可表示为$A^{[0]}$
		







* 多样本的第1层（隐藏层）
		* $Z^{[1]}$表示 [$z^{[1](i)}$...,$z^{[1](m)}$]
		* $A^{[1]}$表示 [$a^{[1](i)}$...,$a^{[1](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有隐藏单元。
	* 多样本的第2层（输出层）
		* $Z^{[2]}$表示 [$z^{[2](i)}$...,$z^{[2](m)}$]
		* $A^{[2]}$表示 [$a^{[2](i)}$...,$a^{[2](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有隐藏单元。



### 3.6 激活函数（ Activation functions）
		
&emsp;　使用一个神经网络时，需要决定使用哪种激活函数隐藏层上,哪种用在输出节点上。到目前为止，之的视频只用过sigmoid激活函数，但是有时其他的激活函数效果会更好。
* ##### tanh函数
	* tanh函数或者双曲正切函数是总体上都优于sigmoid函数的 激活函数。
	* tanh函数公式，它的值域在1至-1之间：
		$a=tanh(z)= \frac{e^z-e^{-z}}{e^z+e^{-z}}$
	* &emsp;　事实上 ，tanh函数是函数 sigmoid的向下平移和伸缩后的结果。对它进行了变形，穿过了 (0,0)点，并且值域介于 +1和-1之间 。
	* &emsp;　结果表明， 如果在隐藏层上 使用函数g(z^[1]^)=tanh(z^[1]^)效果总是优于 sigmoid函数 。 因为 函数值域在-1和+1的激活函数， 其均值是更接近零。在训练一个算法模型时如果使用 tanh函数 代替 sigmoid函数中心化数据，使得的平均值更接近 0而不是 而不是 0.5。
	* &emsp;　往后我们会越来多使用tanh函数代替sigmoid函数，因为他的表现比sigmoid比好。
	* &emsp;　但有一个 但有一个 例外 ：在二分类的问题中， 对于输出层：在二分类的问题中，对于输出层因为 Y的值 是 0或 1，所以想让 y(hat)的数值介于 的数值介于0和 1之间 ，而不是在-1和+1之间 。需要使用 sigmoid激活函数。这里的 g(z[2]) 是等于 $\sigma(z[2])$。在这个例子里看到的是，对隐藏层使用 tanh激活函数,输出层使用sigmoid函数 。

<br>

* ##### ReLu函数(修正线性单元函数)
	* sigmoid函数和 tanh函数两者共同的缺点是在 z特别大或者特别小的情况下导数的梯度或者函数的斜率会变得特别小，最后就接近于 0，导致降低了梯度下降的速度。 
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTA5NDg1MTk1NywtNjk5MzQwOTM0LDE1NT
c3MDU2MjcsLTEwODc3NjY3NTFdfQ==
-->