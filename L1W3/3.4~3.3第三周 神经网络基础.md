
# 3.4~3.5第三周 神经网络基础
[TOC]

### 3.4 多样本向量化（Vectorizing across multiple examples）

* ##### 多样本的神经网络
	* 逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的。
	* $x^{(1)}---->a^{[2](1)}=\hat{y}^{(1)}$
	* $x^{(2)}---->a^{[2](2)}=\hat{y}^{(2)}$
	* ...
	* $x^{(m)}---->a^{[2](m)}=\hat{y}^{(m)}$

* ##### 非向量化的形式实现多样本
	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$

* ##### 向量化的形式实现多样本
	* 多样本的第0层（输入层）
		 * 定义矩阵X 等于训练样本，将它们组合成矩阵的各列，形成一个（nx,m）维矩阵，其中nx表示单个样本的特征向量数量，m表示训练样本数量。
		 * 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有特征向量。
	* 多样本的第1层（隐藏层）
		* $W^{[1]}$ 表示[$w_1^{[1]}$,$w_2^{[1]}$,$w_3^{[1]}$，$w_4^{[1]}$]的矩阵，其中每个$w_i^{[1]}$对应单个样本的所有特征向量，所以$W^{[1]}$是一个（隐藏层神经元数，样本特征向量数）矩阵
		* $Z^{[1]}$表示 [$z^{[1](1)}$...,$z^{[1](m)}$]
		* $A^{[1]}$表示 [$a^{[1](1)}$...,$a^{[1](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有隐藏层神经单元。
	* 多样本的第2层（输出层）
		* $W^{[2]}$ 表示[$w_1^{[2]}$]的矩阵，其中每个$w_i^{[2]}$长度对应隐藏层有多少个神经元输出结果，所以$W^{[2]}$是一个（输出层神经元数，隐藏层输出结果数）矩阵
		* $Z^{[2]}$表示 [$z^{[2](1)}$...,$z^{[2](m)}$]
		* $A^{[2]}$表示 [$a^{[2](1)}$...,$a^{[2](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有输出层神经单元。
<br>

### 3.5 向量化实现的解释（Justification for vectorized implementation）
* #####  以$Z^{[1]}=W^{[1]}X+b^{[1]}$为例子解释向量化
	* 我们先手动对几个样本计算一下前向传播，看有什么规律：
	* $z^{[1](1)}=W^{[1]}x^{(1)}+b^{[1]}$
	* $z^{[1](2)}=W^{[1]}x^{(2)}+b^{[1]}$
	* $W^{[1]}$是一个（4，nx）矩阵，4是由隐藏层有多少个神经单元决定，每个样本$x^{(1)},x^{(2)}$都是列向量，根据矩阵乘法，矩阵乘以列向量得到列向量:
	* ![730faaf8.png](:storage\a7715ab7-7d65-4d2b-8e04-9333d8850de0\730faaf8.png)
	* 这些列向量就组成了$W^{[1]}$点乘X=$Z^{[1]}$矩阵
	* $Z^{[1]}$表示 [$z^{[1](1)}$...,$z^{[1](m)}$]，每个$z^{[1](i)}$都是一个列向量
	* 最后往矩阵每个列向量加实数$b^{(i)}$，利用Python广播原理，$b^{(i)}$会加到每个列向量的每一个元素里,而$b^{(i)}$也组成了矩阵$b^{[1]}$。

<br>

* ##### 推广到其他公式
	* 上面我们论证了把for循环里，$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$公式向量化为$Z^{[1]}=W^{[1]}X+b^{[1]}$，同理可推出其余公式的向量化：
	* 	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$
	* 向量化：
		* $Z^{[1]}=W^{[1]}X+b^{[1]}$
		* $A^{[1]}=\sigma(Z^{[1]})$
		* $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
		* $A^{[2]}=\sigma(Z^{[2]})$
		* 其中X也可表示为$A^{[0]}$
		








<!--stackedit_data:
eyJoaXN0b3J5IjpbLTY5OTM0MDkzNCwxNTU3NzA1NjI3LC0xMD
g3NzY2NzUxXX0=
-->