# 3.4~3.3第三周 神经网络基础
[TOC]

### 3.4 多样本向量化（Vectorizing across multiple examples）

* 多样本的神经网络
	* 逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的。
	* $x^{(1)}---->a^{[2](1)}=\hat{y}^{(1)}$
	* $x^{(2)}---->a^{[2](2)}=\hat{y}^{(2)}$
	* ...
	* $x^{(m)}---->a^{[2](m)}=\hat{y}^{(m)}$

* 非向量化的形式实现多样本
	for i=1 to m:
&emsp;&emsp; $z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$

* 向量化的形式实现多样本
	* 多样本的第0层（输入层）
		 * 定义矩阵X 等于训练样本，将它们组合成矩阵的各列，形成一个（nx,m）维矩阵，其中nx表示单个样本的特征向量数量，m表示训练样本数量。
	* 多样本的第1层（隐藏层）
		* $Z^{[1]}$表示 [$z^{[1](i)}$...,$z^{[1](m)}$]
		* $A^{[1]}$表示 [$a^{[1](i)}$...,$a^{[1](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有隐藏单元。
	* 多样本的第2层（输出层）
		* $Z^{[2]}$表示 [$z^{[2](i)}$...,$z^{[2](m)}$]
		* $A^{[2]}$表示 [$a^{[2](i)}$...,$a^{[2](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有隐藏单元。
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTU1NzcwNTYyNywtMTA4Nzc2Njc1MV19
-->