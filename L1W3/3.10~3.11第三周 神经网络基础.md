
# 3.10~3.11第三周 神经网络基础
[TOC]

### 3.10（选修）直观理解反向传播（Backpropagation intuition）

* ##### 神经网络前向步骤
	![7445b6f0.png](:storage\753f0f4b-b206-4288-829a-fd6076b119d1\7445b6f0.png)	
	* 与逻辑回归类似，代入到神经网络理论，看成是有一个输入层，一个隐藏层，一个输出层。
	* 把$z=w^Tx+b$看作是隐藏层单个神经单元。$a=\sigma(z)$看作是输出层的神经单元，最后求这个神经网络的损失函数，做梯度下降。

* ##### 神经网络后向步骤
	![16f7c6d9.png](:storage\753f0f4b-b206-4288-829a-fd6076b119d1\16f7c6d9.png)
	* 实现后向传播有个技巧，就是要保证矩阵的维度相互匹配。

<br>

### 3.11 随机初始化（ 随机初始化（ 随机初始化（ 随机初始化（ 随机初始化（ 随机初始化（ Random+Initialization）
* 假设神经网络有两个输入特征， 即 $n^{[0]}$等于 2，2个隐藏层单元 n[1]就等于 2。因 此与一个隐藏层相关的矩阵W[1]是 2\*2的矩阵， 假设把它初始化为 0的 2*2矩阵， b[1]也等于 [0 0]^T，把偏置项 b初始化为 0是合理的，但是把 w初始化为0就有问题了。
* 假设W[1]是全0矩阵
	* 那么隐藏层的神经单元$a_1^{[1]},a_2^{[1]}$将会完全一样，他们的激活函数也将完全一样。（这里我理解是，就算使用的函数不一样，譬如a_1用tanh,a_2用relu,由于W矩阵是全0作用在X矩阵的结果就是全0，所以用什么函数都是一样结果。）
	* 由于$a_1^{[1]}=a_2^{[1]}$，所以$dz_1^{[1]}=dz_2^{[1]}$，继而得出，dW1矩阵，上下两行元素（即第一个和第二神经单元的w值）是完全相等的。
	* 根据梯度下降公式，每次更新，dW会是一个这样的矩阵，每行有同值做权重W[1]更新把W[1]减去alpha乘上dW,每次迭代后的W[1]仍然是第一行等于第二行。而且不管迭代多少次，计算的内容都不会变。

* ##### 正确的初始化参数方式
	* W初始化的正确做法是随机初始化参数。你应该这么做，把 W[1]设为 np.random.randn((2,2)) (生成高斯分布)，通常再乘上一个小的数，比如0.01，让生成的随机数控制在小于0.01以下。
	* 选择0.01的理由是如果随机数太大，再用tanh或者sigmoid函数激活，得出的结果a值就会波动很大。因此这种情况下你很可能停在tanh/sigmoid函数的平坦区域（z>10或z<-10），这些地方梯度很小也就意味着下降会慢，因此学习也就很慢。
	* 浅层网络没有太多隐藏层，适合用0.01这个常数，但当你训练一个非常深的神经网络，你可能会选择其他的常数作为初始化随机参数，但不论深层或浅层，这个常数都是选择一个较小的数。
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTUxNjgzMjU5N119
-->