# 3.4~3.6第三周 神经网络基础
[TOC]

### 3.4 多样本向量化（Vectorizing across multiple examples）

* ##### 多样本的神经网络
	* 逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的。
	* $x^{(1)}---->a^{[2](1)}=\hat{y}^{(1)}$
	* $x^{(2)}---->a^{[2](2)}=\hat{y}^{(2)}$
	* ...
	* $x^{(m)}---->a^{[2](m)}=\hat{y}^{(m)}$

* ##### 非向量化的形式实现多样本
	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$

* ##### 向量化的形式实现多样本
	* 多样本的第0层（输入层）
		 * 定义矩阵X 等于训练样本，将它们组合成矩阵的各列，形成一个（nx,m）维矩阵，其中nx表示单个样本的特征向量数量，m表示训练样本数量。
		 * 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有特征向量。
	* 多样本的第1层（隐藏层）
		* $W^{[1]}$ 表示[$w_1^{[1]}$,$w_2^{[1]}$,$w_3^{[1]}$，$w_4^{[1]}$]的矩阵，其中每个$w_i^{[1]}$对应单个样本的所有特征向量，所以$W^{[1]}$是一个（隐藏层神经元数，样本特征向量数）矩阵
		* $Z^{[1]}$表示 [$z^{[1](1)}$...,$z^{[1](m)}$]
		* $A^{[1]}$表示 [$a^{[1](1)}$...,$a^{[1](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有隐藏层神经单元。
	* 多样本的第2层（输出层）
		* $W^{[2]}$ 表示[$w_1^{[2]}$]的矩阵，其中每个$w_i^{[2]}$长度对应隐藏层有多少个神经元输出结果，所以$W^{[2]}$是一个（输出层神经元数，隐藏层输出结果数）矩阵
		* $Z^{[2]}$表示 [$z^{[2](1)}$...,$z^{[2](m)}$]
		* $A^{[2]}$表示 [$a^{[2](1)}$...,$a^{[2](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有输出层神经单元。
<br>

### 3.5 向量化实现的解释（Justification for vectorized implementation）
* #####  以$Z^{[1]}=W^{[1]}X+b^{[1]}$为例子解释向量化
	* 我们先手动对几个样本计算一下前向传播，看有什么规律：
	* $z^{[1](1)}=W^{[1]}x^{(1)}+b^{[1]}$
	* $z^{[1](2)}=W^{[1]}x^{(2)}+b^{[1]}$
	* $W^{[1]}$是一个（4，nx）矩阵，4是由隐藏层有多少个神经单元决定，每个样本$x^{(1)},x^{(2)}$都是列向量，根据矩阵乘法，矩阵乘以列向量得到列向量:
	* ![730faaf8.png](attachments\730faaf8.png)
	* 这些列向量就组成了$W^{[1]}$点乘X=$Z^{[1]}$矩阵
	* $Z^{[1]}$表示 [$z^{[1](1)}$...,$z^{[1](m)}$]，每个$z^{[1](i)}$都是一个列向量
	* 最后往矩阵每个列向量加实数$b^{(i)}$，利用Python广播原理，$b^{(i)}$会加到每个列向量的每一个元素里,而$b^{(i)}$也组成了矩阵$b^{[1]}$。

<br>

* ##### 推广到其他公式
	* 上面我们论证了把for循环里，$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$公式向量化为$Z^{[1]}=W^{[1]}X+b^{[1]}$，同理可推出其余公式的向量化：
	* 	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$
	* 向量化：
		* $Z^{[1]}=W^{[1]}X+b^{[1]}$
		* $A^{[1]}=\sigma(Z^{[1]})$
		* $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
		* $A^{[2]}=\sigma(Z^{[2]})$
		* 其中X也可表示为$A^{[0]}$
		





### 3.6 激活函数（ Activation functions）
&emsp;　使用一个神经网络时，需要决定使用哪种激活函数隐藏层上,哪种用在输出节点上。到目前为止，之的视频只用过sigmoid激活函数，但是有时其他的激活函数效果会更好。
* ##### tanh函数
	* tanh函数或者双曲正切函数是总体上都优于sigmoid函数的 激活函数。
	* tanh函数公式，它的值域在1至-1之间：
		$a=tanh(z)= \frac{e^z-e^{-z}}{e^z+e^{-z}}$
	* &emsp;　事实上 ，tanh函数是函数 sigmoid的向下平移和伸缩后的结果。对它进行了变形，穿过了 (0,0)点，并且值域介于 +1和-1之间 。
	* &emsp;　结果表明， 如果在隐藏层上 使用函数g(z^[1]^)=tanh(z^[1]^)效果总是优于 sigmoid函数 。 因为 函数值域在-1和+1的激活函数， 其均值是更接近零。在训练一个算法模型时如果使用 tanh函数 代替 sigmoid函数中心化数据，使得的平均值更接近 0而不是 而不是 0.5。
	* &emsp;　往后我们会越来多使用tanh函数代替sigmoid函数，因为他的表现比sigmoid比好。
	* &emsp;　但有一个 但有一个 例外 ：在二分类的问题中， 对于输出层：在二分类的问题中，对于输出层因为 Y的值 是 0或 1，所以想让 y(hat)的数值介于 的数值介于0和 1之间 ，而不是在-1和+1之间 。需要使用 sigmoid激活函数。这里的 g(z[2]) 是等于 $\sigma(z[2])$。在这个例子里看到的是，对隐藏层使用 tanh激活函数,输出层使用sigmoid函数 。

<br>

* ##### ReLu函数
	![d788d43f8794a4c25b5e4dd902f41bd5ac6e39c6.jpg](https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike92%2C5%2C5%2C92%2C30/sign=ecc0fafa00d79123f4ed9c26cc5d32e7/d788d43f8794a4c25b5e4dd902f41bd5ac6e39c6.jpg)
&emsp;　sigmoid函数和 tanh函数两者共同的缺点是在 z特别大或者特别小的情况下导数的梯度或者函数的斜率会变得特别小，最后就接近于 0，导致降低了梯度下降的速度。 
&emsp;　只要是正值的情况下导数恒等于 1，当是负值的时候 ，导数恒等于 0。实际上来说， 当使用z的导数时， z=0的导数是没有定义的。但是当编程实现的时候， z的取值刚好等于 0.0000000，这个值相当小， ， 这个值相当小，这个值相当小所以 ，在实践中不需要担心这个值 ，z是等于 0的时候 ，假设一个导数 是 1或者 0效果都可以。
	
* ##### 选择激活函数的经验法则
	* &emsp;　如果输出是 如果输出是 0 1值（二分类问题），则 （二分类问题），则输出层选择 sigmoid函数 ，然后其它的所有单元都选择 ReLu函数 。
	* &emsp;　这是很多激活函数的默认选择， 如果不确定使用哪个激活函数 不确定使用哪个激活函数那么通常会 使用 Relu激活函数。有时tanh激活函数但 Relu的一个优点是 的一个优点是当 z是负值的时候，导数等于 0。
	
* ##### 另一个ReLu函数 Leaky Relu函数
	* ![blog-prelu.png](http://7pn4yt.com1.z0.glb.clouddn.com/blog-prelu.png)
	当 z是负值时，这个函数的不等于 0，而是轻微的倾斜。

*  ##### 两种Relu函数的优点
	*  &emsp;两者的优点是：在 z的区间变动很大情况下，激活函数的导或者斜率都 会远大于0。所以，在实践中 ，使用 ReLu激活函数神经网络通常会比使用 sigmoid或者 tanh激活函 数学习的更快。主要原因是：在训练过程中， sigmoid或者tanh激活函数梯度下降为0，从而减慢学习速度，Relu函数则不会。
	*  &emsp;前提是，z不能小于等于0，只要有足够多的隐藏层保证z大于0就可以。

* ##### 激活函数总结
	* sigmoid激活函数 ：除了输出层是一个二分类问题基本不会用它。
	* tanh激活函数 激活函数 ：因为是非常优秀的，几乎适合所有场。 
	* ReLu激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu或者 Leaky ReLu。公式： 𝑎=max(0.01𝑧,𝑧)
	


<!--stackedit_data:
eyJoaXN0b3J5IjpbMTkyMDI0OTU3M119
-->