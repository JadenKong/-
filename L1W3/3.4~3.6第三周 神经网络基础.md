# 3.4~3.6第三周 神经网络基础
[TOC]

### 3.4 多样本向量化（Vectorizing across multiple examples）

* ##### 多样本的神经网络
	* 逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进行的。
	* $x^{(1)}---->a^{[2](1)}=\hat{y}^{(1)}$
	* $x^{(2)}---->a^{[2](2)}=\hat{y}^{(2)}$
	* ...
	* $x^{(m)}---->a^{[2](m)}=\hat{y}^{(m)}$

* ##### 非向量化的形式实现多样本
	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$

* ##### 向量化的形式实现多样本
	* 多样本的第0层（输入层）
		 * 定义矩阵X 等于训练样本，将它们组合成矩阵的各列，形成一个（nx,m）维矩阵，其中nx表示单个样本的特征向量数量，m表示训练样本数量。
		 * 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有特征向量。
	* 多样本的第1层（隐藏层）
		* $W^{[1]}$ 表示[$w_1^{[1]}$,$w_2^{[1]}$,$w_3^{[1]}$，$w_4^{[1]}$]的矩阵，其中每个$w_i^{[1]}$对应单个样本的所有特征向量，所以$W^{[1]}$是一个（隐藏层神经元数，样本特征向量数）矩阵
		* $Z^{[1]}$表示 [$z^{[1](1)}$...,$z^{[1](m)}$]
		* $A^{[1]}$表示 [$a^{[1](1)}$...,$a^{[1](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有隐藏层神经单元。
	* 多样本的第2层（输出层）
		* $W^{[2]}$ 表示[$w_1^{[2]}$]的矩阵，其中每个$w_i^{[2]}$长度对应隐藏层有多少个神经元输出结果，所以$W^{[2]}$是一个（输出层神经元数，隐藏层输出结果数）矩阵
		* $Z^{[2]}$表示 [$z^{[2](1)}$...,$z^{[2](m)}$]
		* $A^{[2]}$表示 [$a^{[2](1)}$...,$a^{[2](m)}$]
		* 从水平上看，代表了各个训练样本。从竖直上看，代表了单个样本的所有输出层神经单元。
<br>

### 3.5 向量化实现的解释（Justification for vectorized implementation）
* #####  以$Z^{[1]}=W^{[1]}X+b^{[1]}$为例子解释向量化
	* 我们先手动对几个样本计算一下前向传播，看有什么规律：
	* $z^{[1](1)}=W^{[1]}x^{(1)}+b^{[1]}$
	* $z^{[1](2)}=W^{[1]}x^{(2)}+b^{[1]}$
	* $W^{[1]}$是一个（4，nx）矩阵，4是由隐藏层有多少个神经单元决定，每个样本$x^{(1)},x^{(2)}$都是列向量，根据矩阵乘法，矩阵乘以列向量得到列向量:
	* ![730faaf8.png](attachments\730faaf8.png)
	* 这些列向量就组成了$W^{[1]}$点乘X=$Z^{[1]}$矩阵
	* $Z^{[1]}$表示 [$z^{[1](1)}$...,$z^{[1](m)}$]，每个$z^{[1](i)}$都是一个列向量
	* 最后往矩阵每个列向量加实数$b^{(i)}$，利用Python广播原理，$b^{(i)}$会加到每个列向量的每一个元素里,而$b^{(i)}$也组成了矩阵$b^{[1]}$。

<br>

* ##### 推广到其他公式
	* 上面我们论证了把for循环里，$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$公式向量化为$Z^{[1]}=W^{[1]}X+b^{[1]}$，同理可推出其余公式的向量化：
	* 	for i=1 to m:
&emsp;&emsp;$z^{[1]}=w^{[1]}x^{(i)}+b^{[1]}$
&emsp;　$a^{[1](i)}=\sigma(z^{[1](i)})$
&emsp;　$z^{[2](i)}=w^{[2]}a^{[1](i)}+b^{[2]}$
&emsp;　$a^{[2](i)}=\sigma(z^{[2](i)})$
	* 向量化：
		* $Z^{[1]}=W^{[1]}X+b^{[1]}$
		* $A^{[1]}=\sigma(Z^{[1]})$
		* $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
		* $A^{[2]}=\sigma(Z^{[2]})$
		* 其中X也可表示为$A^{[0]}$
		





### 3.6 激活函数（ Activation functions）
* ##### ReLu函数
	![d788d43f8794a4c25b5e4dd902f41bd5ac6e39c6.jpg](https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike92%2C5%2C5%2C92%2C30/sign=ecc0fafa00d79123f4ed9c26cc5d32e7/d788d43f8794a4c25b5e4dd902f41bd5ac6e39c6.jpg)
&emsp;　只要是正值的情况下导数恒等于 1，当是负值的时候 ，导数恒等于 0。实际上来说， 当使用z的导数时， z=0的导数是没有定义的。但是当编程实现的时候， z的取值刚好等于 0.0000000，这个值相当小， ， 这个值相当小，这个值相当小所以 ，在实践中不需要担心这个值 ，z是等于 0的时候 ，假设一个导数 是 1或者 0效果都可以。
	
* ##### 选择激活函数的经验法则
	* &emsp;　如果输出是 如果输出是 0 1值（二分类问题），则 （二分类问题），则输出层选择 sigmoid函数 ，然后其它的所有单元都选择 ReLu函数 。
	* &emsp;　这是很多激活函数的默认选择， 如果不确定使用哪个激活函数 不确定使用哪个激活函数那么通常会 使用 Relu激活函数。有时tanh激活函数但 Relu的一个优点是 的一个优点是当 z是负值的时候，导数等于 0。
	
* ##### 另一个ReLu函数 Leaky Relu函数
	* ![blog-prelu.png](http://7pn4yt.com1.z0.glb.clouddn.com/blog-prelu.png)
	当 z是负值时，这个函数的不等于 0，而是轻微的倾斜。

*  ##### 两种Relu函数的优点
	*  &emsp;两者的优点是：在 z的区间变动很大情况下，激活函数的导或者斜率都 会远大于0。所以，在实践中 ，使用 ReLu激活函数神经网络通常会比使用 sigmoid或者 tanh激活函 数学习的更快。主要原因是：在训练过程中， sigmoid或者tanh激活函数梯度下降为0，从而减慢学习速度，Relu函数则不会。
	*  &emsp;前提是，z不能小于等于0，只要有足够多的隐藏层保证z大于0就可以。

* ##### 激活函数总结
	* sigmoid激活函数 ：除了输出层是一个二分类问题基本不会用它。
	* tanh激活函数 激活函数 ：因为是非常优秀的，几乎适合所有场。 
	* ReLu激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu或者 Leaky ReLu。公式： 𝑎=max(0.01𝑧,𝑧)
	


