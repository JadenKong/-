# 3.7~3.9第三周 神经网络基础
[TOC]

### 3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）
* 线性函数
	*  线性函数就是想y=wx+b这样的函数。
	*  如果把神经网络隐藏层中的非线性函数都换成线性函数：
		* $Z^{[1]}=W^{[1]}X+b^{[1]}$
		* $A^{[1]}=Z^{[1]}$
		* $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
		* $A^{[2]}=Z^{[2]}$
	* 2式代入3式可得出：		$A^{[2]}=W^{[2]}W^{[1]}X+W^{[2]}b^{[1]}+b^{[2]}$
	* $W^{[2]}W^{[1]}$看作新的W,$W^{[2]}b^{[1]}+b^{[2]}$看作新的b,可看出，两层线性函数等价一个新的线性函数，从模型复杂度看，A1隐藏层的线性函数是多余的，所以隐藏层是线性函数没有任何意义，除非改为引用非线性函数sigmoid Relu等做隐藏层函数。
	* 只有在线性回归中y的取值范围是实数，不是二分类或一定范围的值，才有可能在输出层用线性函数，这样输出的预测值$\hat{y}$也会是一个实数。
* #####总结
	* 总而言之，不能在隐藏层用线性激活函数，可以用ReLU 或者tanh 或者leaky ReLU 或者
其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层

	
### 3.8 激活函数的导数（Derivatives of activation functions）
* ##### sigmoid函数的导数
	![fb381722.png](:storage\b491a883-d950-4d79-bd0d-7dd0b829d3c4\fb381722.png)
	$$sigmoid(z)^\prime=\frac{dg(z)}{dz}=(\frac{1}{1+e^{-z}})^\prime=g(z)(1-g(z))$$
	* 当z=10或z=-10时，$\frac{dg(z)}{dz} \approx0$
	* 当z=0时，$\frac{dg(z)}{dz}=\frac{1}{4}$
<br>
* ##### tanh函数的导数
	![bd141dd2.png](:storage\b491a883-d950-4d79-bd0d-7dd0b829d3c4\bd141dd2.png)
	$$g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$
	$$\frac{dg(z)}{dz}={tanh(z)}^\prime={\frac{e^z-e^{-z}}{e^z+e^{-z}}}^\prime=1-g(z)^2$$
	* 当z=10或z=-10时，$\frac{dg(z)}{dz}\approx0$
	* 当z=0时，$\frac{dg(z)}{dz}=1$

<br>

* ##### ReLu和Leaky ReLu函数
	![e5f9c560.png](:storage\b491a883-d950-4d79-bd0d-7dd0b829d3c4\e5f9c560.png)
	* ReLU函数
		$$g(z)=max(0,z)$$
		当z<0时，$g(z)=0,g(z)^\prime=0$
		当z>0时，$g(z)=z,g(z)^\prime=1$	
		当z=0时，$g(z)undefined,g(z)^\prime$值为undefined
	* Leaky ReLu函数
		$$g(z)=max(0.01z,z)$$
		当z<0时，$g(z)=0.01z,g(z)^\prime=0.01$
		当z>0时，$g(z)=z,g(z)^\prime=1$	
		当z=0时，$g(z)undefined,g(z)^\prime$值为undefined
		
<br>

### 3.9 神经网络的梯度下降（ Gradient descent for neural networks）

* ##### 假设隐藏层只有一个神经单元的情况
	* 这种情况会有$W^{[1]},b^{[1]},W^{[2]},b^{[2]}$这些参数，其中$nx=n^{[0]}$表示输入特征个数，$n^{[1]}$表示隐藏层单元个数，$n^{[2]}$表示输出单元个数，由于隐藏层只有一个神经单元，代表g^[1]^只包含一个激活函数。
	* $W^{[1]}$维度是（n1,n0）$,b^{[1]}$是（n1,1）$,W^{[2]}$是（n2,n1）$,b^{[2]}$是（n2,1）
	* 此情况下的成本函数：
		$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]})=\frac{1}{m}\sum_{i=1}^{m}L(\hat{y},y)$
	* 梯度下降
		* $W^{[1]}:= W^{[1]}-\alpha*d W^{[1]}$
		* $b^{[1]}:= b^{[1]}-\alpha*d b^{[1]}$
		* $W^{[2]}:= W^{[2]}-\alpha*d W^{[2]}$
		* $b^{[2]}:= b^{[2]}-\alpha*d b^{[2]}$
	
	<br>
	
* ##### 隐藏层只有一个神经单元的后向传播过程
	![33260db0.png](:storage\b491a883-d950-4d79-bd0d-7dd0b829d3c4\33260db0.png)
	左边是前向，右边是反向
	* 引用逻辑回归用的推导求dZ,dW,dB公式
		* 对损失函数$L(a,y) = -ylog(a) - (1-y)log(1 -a)$ 求a(即输出$\hat{y}$)的偏导：
		$$L'(a,y) =\frac{dL(a,y)}{da}= -\frac{y}{a}+\frac{1-y}{1-a}$$
		* 现在再反向一步，根据步骤2算出z对函数L的求导$\frac{dL}{dz}$
		* 因为$\frac{dL}{dz}=\frac{dL}{da}\frac{da}{dz},\frac{da}{dz}=a(1-a)$，***注意***：这里对a函数的z求导结果，用a来表示，方便与前面dL/da的结果计算。
			* 所以
			$$\frac{dL}{dz}= \frac{dL}{da}\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})(a(1-a))=a-y$$
	* 将以上推导过程代入到神经网络，$\frac{dL}{dz}=A^{[2]}-Y$
	<br>
	* 根据多样本下且W为多特征矩阵，b为常数的形式：
		* $\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$

		* $\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$
	* 将以上推导过程代入到神经网络：
		* $dw^{[2]}=\frac{1}{m}A^{[1]}(dZ^{[2]})^T$
		* $db^{[2]}=\frac{1}{m}\sum_{i=1}^m (dZ^{[2]})$，在python中代码表示为，$db^{[2]}=1/m*np.sum(dz^{[2]},axis=1,keepdims=True)$
		* 这里np.sum 是python 的numpy 命令，axis 等于1 表示水平相加求和，keepdims是防止python输出那些古怪的秩数(n,1)，加上这个确保阵矩阵$db^{[2]}$ 这个向量输出的维度为(n,1)这样标准的形式。
	
	*  dZ1,dW1,db1的推导 
		* $dZ^{[1]}=[(W^{[2]})^T*dZ^{[2]} ] time [(g^{[1]})^\prime*Z^{[1]}]$
		* $(g^{[1]})^\prime$表示隐藏层用的函数的导数，由于隐藏层只有一个神经单元，代表$g^{[1]}$只包含一个激活函数。
		* time表示逐元素乘积，把两边两个矩阵逐元素相乘
		* 然后根据dZ1求dW1,db1
		* $dw^{[1]}=\frac{1}{m}X(dZ^{[1]})^T$
		*  $db^{[1]}=\frac{1}{m}\sum_{i=1}^m (dZ^{[1]})$，在python中代码表示为，$db^{[2]}=1/m*np.sum(dz^{[1]},axis=1,keepdims=True)$
		* axis=0，表示按列相加 axis=1，表示按行相加，keepdims为True确保计算后的矩阵不变形













<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4ODEzMjI0NywxNTY5OTk1MDM1XX0=
-->