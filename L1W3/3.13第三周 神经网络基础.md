# 3.13第三周 神经网络基础
[TOC]

* ##### 神经网络的数学步骤

![dd](D:/Python/Kaggle/L1神经网络和深度学习/第一课第三周编程作业/assignment3/images/classification_kiank.png)

For one example $x^{(i)}$:
$$z^{[1] (i)}=W^{[1]} x^{(i)}+b^{[1] (i)}$$ 
$$a^{[1] (i)} = \tanh(z^{[1] (i)})$$
$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}$$
$$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})$$

$$y^{(i)}_{prediction} =  \begin{cases} 
1, & if　 a^{[2](i)} > 0.5 \\ 0, & otherwise 
\end{cases}$$

Given the predictions on all the examples, you can also compute the cost J as follows: 
$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small $$

* ##### 神经网络的编程步骤
	1. 定义神经网络结构(输入单元数,隐藏单元数等)。
	2. 初始化模型的参数
	3. 循环下列步骤：
		1. 实现前向传播
		2. 计算损失函数
		3. 实现向后传播以获得更新值    
		4. 更新参数（梯度下降）
 
TODO：测试隐藏层4个神经单元，如果X相同，W初始化不是随机矩阵而是全1矩阵，4个神经单元的输出是否一样，Z1矩阵经过tanh()函数会输出什么？
答：隐藏层4个单元都是用tanh()函数，所以如果对这4个单元输入相同的参数，输出结果是一样的。


* ##### 计算损失函数
	$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small$$


logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),1-Y)
    cost = - np.sum(logprobs)/m
    #no need to use a for loop!不需使用循环就可实现，注意这个是逐元素相乘，不是点乘。把矩阵相乘和累加分两步完成


* ##### 后向传播
![](D:/Python/Kaggle/L1神经网络和深度学习/第一课第三周编程作业/assignment3/images/grad_summary.png)

	$$$计算dZ1 需要计算 g^{[1]'}(Z^{[1]})由于g^{[1]}(z) 是tanh函数, 如果a=g^[1](z)然后g^{[1]'}(z) = 1-a^2所以代码可写成g^{[1]'}(Z^{[1]})利用(1 - np.powevr(A1, 2))$$$
	* np.power(A1,2)是矩阵n次方计算
	* db1 = np.sum(dZ1,axis=1,keepdims=True)
		* 注意sum函数有axis=1，表示相加方式是每行元素累加成一个结果，即把多列矩阵压缩成单列矩阵。
		* 这种做法的意义是，累加要保持b的形状，b矩阵的行数代表该层的单元数，每列代表各个样本的b。根据公式，db的原意是各个样本的dz值相加之后除以m得平均值，所以要每行相加。

<br>

* ##### 预测函数predict(parameters, X):
```
 A2, cache = forward_propagation(X, parameters)
# 统计输出结果A2里大于0.5的元素的占百分比
predictions = A2>0.5
```

* ##### 总结函数nn_model
	第8000次之后cost值开始与答案不符合，网上答案也与作业答案不符合。10000次梯度下降得出的W1,W2,b1,b2矩阵每个元素值的最后两三位和答案不相符，但不影响最后预测图形色块结果。
    * 使用神经网络的预测结果：
![](D:/Python/Kaggle/L1神经网络和深度学习/第一课第三周编程作业/assignment3/images/使用神经网络的预测结果.png)


* ##### 隐藏层神经单元数的选择
	实验证明，hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]隐藏层单元数在5-10时表现最好，到20个单元时已经出现过拟合
    
<br>

* ##### 神经网络在其他数据集的表现
![](D:/Python/Kaggle/L1神经网络和深度学习/第一课第三周编程作业/assignment3/images/神经网络在其他数据集的表现.png)
	以上4个神经网络均用5个神经单元的隐藏层 n_h=5
