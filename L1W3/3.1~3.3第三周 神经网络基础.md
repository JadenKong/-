# 3.1~3.3第三周 神经网络基础
[TOC]

### 3.1 神经网络概述（Neural Network Overview）
* 上周回顾
	逻辑回归损失函数：
	![b7447324.png](attachments\b7447324.png)
* 神经网络
	![4fd433fa.png](attachments\4fd433fa.png)
	* 我们会使用新的符号[1]（上标方括号1）表示与这些节点相关的数，这些节点被称为一层。我们用[2]（上标方括号2）表示和这个节点相关的数，这是网络的第二层。
	* 第一层类似逻辑回归，
		* $z^{[1]}=w^{[1]}X+b^{[1]}$
		* $a^{[1]}$=$\sigma(z^{[1]})$
	* 接下来第二层你需要使用另外一个线性方程以$a^{[1]}$为输入值，计算出新的输出
		*  $z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}$
		*  $a^{[2]}$=$\sigma(z^{[2]})$
	* 此时$a^{[2]}$就是整个神经网络最终的输出，用$\hat{y}$表示网络的输出。
		* $\hat{y}$=$a^{[2]}$
	* 得到最终输出后再对这个输出做损失函数
		* $L(a^{[2]},y)$
	![22921e62.png](attachments\22921e62.png)
	* 对损失函数做梯度下降，反向计算导数da,dz；然后是导数$dw^{[2]},db^{[2]}$,然后是$dw^{[1]},db^{[1]}$
	* ![23affba5.png](attachments\23affba5.png

<br>

### 3.2 神经网络的表示（Neural Network Representation）
*	**只包含一个隐藏层的神经网络**
	![15a67244.png](attachments\15a67244.png)
	* **输入层**
&emsp;　输入层是由输入特征x1,x2,x3,竖直堆叠起来，，就像我们之前用向量x 表示输入特征。这里有个可代替的记
号可以用来表示输入特征。我们用$a^{[0]}$来表示。这个a 也表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中。
	* **隐藏层**
		* 隐藏层由4个结点组成,隐藏层的含义：在一个神经网
络中，当你使用监督学习训练它的时候，训练集包含了输入x 也包含了目标输出y,所以术语隐藏层的含义是在训练集中,这些中间结点的准确值我们是不知道的，你看不见它们在训练集中应具有的值。
		* 隐藏层也同样会产生一些激活值，那么我将其记作$a^{[1]}$。具体地，这里的第一个单元或结点我们将其表示为$a_1^{[1]}$,第二个结点的值我们记$a_2^{[1]}$，如此类推。
		* 在python代码中的表示:它是一个规模为4x1 的矩阵或一个大小为4 的列向量。，因为在本例中，我们有四个结点或者单元，或者称为四个隐藏层单元。
	* **输出层**
		* 输出层负责产生预测值$\hat{y}$
		* 最后输出层将产生某个数值a，它只是一个单独的实数，所以$\hat{y}$值将取为$a^{[2]}$。
	* **隐藏层和输出层的参数w和b**
		* 我们要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个参数w和b ，我将给它们加上上标$w^{[1]}$和$b^{[1]}$表示这些参数是和第一层有关系的，也就是和隐藏层有关系。
		* 代码中我们会看到w是一个4x3 的矩阵，而b 是一个4x1的向量，第一个数字4 源自于我们有四个结点或隐藏层单元，然后数字3源自于这里有三个输入特征x1,x2,x3
		* 相似的输出层也有一些与之关联的参数$w^{[2]}$和$b^{[2]}$从规模上来看，它们的规模分别是1x4 以及1x1。1x4 是因为隐藏层有四个隐藏层单元而输出层只有一个单元.
		* ![09d4313b.png](attachments\09d4313b.png)
<br>
	* 备注
		* 当我们计算网络的层数时，输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。


### 3.3 计算一个神经网络的输出（Computing a Neural Network'soutput）
* **逻辑回归LR回顾**
	* 逻辑回归LR 的计算有两个步骤，首先你按步骤计算出Z，然后在第二步中你以sigmoid 函数为激活函数计算Z（得出a），一个神经网络只是这样子做了好多次重复计算。
	* ![57af5245.png](attachments\57af5245.png)
	* 在神经网络里，这个逻辑回归LR同时也是一个神经元的计算内容

* **神经网络中每个神经元的计算内容**
	* ![9130b997.png](attachments\9130b997.png)
	* 第一层第一个神经元$z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},a_1^{[1]}=\sigma(z_1^{[1]})$
	* 第一层第二个神经元$z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]},a_2^{[1]}=\sigma(z_2^{[1]})$
	* 第一层第三第四个神经元如此类推...
<br>

* **向量化神经网络第0层和第一层**
	* 向量化输入层$a^{[0]}$
		* 与之前逻辑回归类似，可以把输入层每个特征向量X1,X2,X3看成一个（3,1）的列向量。记作$a^{[0]}$
	* 向量化隐藏层$a^{[1]}$中神经元公式的w参数
		* 把第一层神经网络的1~4神经元公式的w参数都提出来，已知神经元有4个，每个神经元的w都对应3个输入特征向量，所以第一层网络的w参数可以向量化为一个（4,3）矩阵。记作$w^{[1]}$
	* 向量化隐藏层$a^{[1]}$中神经元公式的b参数
		* 把第一层神经网络的1~4神经元公式的w参数都提出来，已知神经元有4个，每个神经元的b都是一个实数，所以第一层网络的b参数可以向量化为一个（4,1）矩阵。记作$b^{[1]}$
	* 向量化隐藏层$a^{[1]}$中神经元公式逻辑回归结果z
		* 根据四条神经元的逻辑回归公式，$z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]}$...，把结果z向量化，由于有4个神经元，向量化为（4,1）矩阵。记作$z^{[1]}$
	* 向量化隐藏层$a^{[1]}$中神经元公式逻辑回归结果sigmoid化结果a
		* 根据四条神经元的逻辑回归公式，$a_1^{[1]}=\sigma(z_1^{[1]})$...把结果a向量化，由于有4个神经元，向量化为（4,1）矩阵。记作$a^{[1]}$
	* **神经网络第一层向量化结果**
		* $z^{[1]}=w^{[1]}x+b^{[1]}$
		* $a^{[1]}=\sigma(z^{[1]})$

* **向量化神经网络第二层**
	* 向量化第二层$a^{[2]}$中神经元公式的w参数
		* 已知第一层结果输出4个结果$a_1^{[1]}..a_4^{[1]}$，对第二层来说就是有4个输入参数，而第二层只有一个神经元，所以第二层网络的w参数可以向量化为一个（1，4）矩阵。记作$w^{[2]}$
	* 向量化隐藏层$a^{[2]}$中神经元公式的b参数
		* 由于第二层只有一个神经元，所以第二层网络的b参数可以向量化为一个（1,1）矩阵。记作$b^{[2]}$
	* 向量化隐藏层$a^{[2]}$中神经元公式逻辑回归结果z
		* 第二层只有一条神经元的逻辑回归公式，$z_1^{[2]}=w_1^{[2]T}x+b_1^{[2]}$，把结果z向量化为（1,1）矩阵。记作$z^{[2]}$
	* 向量化隐藏层$a^{[2]}$中神经元公式逻辑回归结果sigmoid化结果a
		* 第二层只有一条神经元的逻辑回归公式，$a_1^{[2]}=\sigma(z_1^{[2]})$把结果a向量化为（1,1）矩阵。记作$a^{[2]}$
	* **神经网络第二层向量化结果**
		* $z^{[2]}=w^{[2]}a^{[1]}+b^{[2]}$
		* $a^{[2]}=\sigma(z^{[2]})$
* 图片描述
	* ![82b32c05.png](attachments\82b32c05.png)
	* ![265a97a2.png](attachments\265a97a2.png)
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTg4MTY4ODQ0NF19
-->