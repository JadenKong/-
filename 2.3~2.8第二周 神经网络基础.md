# 2.3~2.8第二周 神经网络基础
[TOC]

### 2.4梯度下降
* 梯度下降法可以做什么
	* 在你测试集上，通过最小化代价函数(成本函数)J(w,b)来训练的参数b和w.
		![3e19cb97.png](:storage\d6750442-a6af-46c6-8f74-ac932b78e8b9\3e19cb97.png)
		![1e76f70b.png](:storage\d6750442-a6af-46c6-8f74-ac932b78e8b9\1e76f70b.png)

	* 在这个图中，横轴表示你的空间参数b和w，代价函数(成本函数) ,J(w,b)是在水平轴w和b上的曲面，因此曲面的高度就是,J(w,b)在某一点的函数值。我们所做的就是找到使得代价函数（成本函数）J(w,b) 函数值是最小值，对应的参数w和b 。
<br>
* 梯度下降法的细节化说明(仅有一个参数的情况)
	假定代价函数（成本函数）J(w,b)只有一个参数w，即用一维曲线代替多维曲线。
	![c47cd9f8.png](:storage\d6750442-a6af-46c6-8f74-ac932b78e8b9\c47cd9f8.png)
	* 迭代就是不断重复做如图的公式:
		$$ w:=w-\alpha\frac{dJ(w)}{dw}$$
		
	*  w:= 表示不断更新的参数w
	* $\alpha$表示学习率(learning rate)，用来控制步长(step),即向下走一步的长度
	* $\frac{dJ(w)}{dw}$ 就是函数J(w)对w求导（derivative）也是(w,J(w))点的切线斜率，在编程中我们会使用dw表示这个结果
	* 当初始点在曲线左边时，斜率为负，$\alpha\frac{dJ(w)}{dw}$值为负,$w-\alpha\frac{dJ(w)}{dw}$ 为一个不断增加的值，所以更新的w值比之前的w值不断增加，向中间最小值靠拢
	* 当初始点在曲线右边时，斜率为正，$\alpha\frac{dJ(w)}{dw}$值为正,$w-\alpha\frac{dJ(w)}{dw}$ 为一个不断减少的值，所以更新的w值比之前的w值不断减少，向中间最小值靠拢
<br>
* 梯度下降法的细节化说明(两个或以上参数的情况)
	* 逻辑回归的代价函数（成本函数） , J(w,b)是含有两个参数的。
	$$ w:=w-\alpha\frac{∂J(w,b)}{∂w}$$
	$$ b:=b-\alpha\frac{∂J(w,b)}{∂w}$$
	* ∂表示求偏导符号,可以读作round,一个多变量的函数的偏导数，就是它关于其中一个变量的导数而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。
	* $\frac{∂J(w,b)}{∂w}$就是函数J(w,b)对w求偏导，在代码中我们会使用dw表示这个结果
	* $\frac{∂J(w,b)}{∂b}$就是函数J(w,b)对b求偏导，在代码中我们会使用dw表示这个结果
	* 小写字母d 用在求导数（derivative），即函数只含一个变量的情况; 偏导数符号∂ 用在求偏导（partial derivative），即函数含有两个以上的参数。
<br>

### 2.5 导数（Derivatives）
![0a0f3110.png](:storage\d6750442-a6af-46c6-8f74-ac932b78e8b9\0a0f3110.png)
*	导数这个概念意味着斜率，导数听起来是一个很复杂的词，但是斜率以一种很友好的方式来描述导数这个概念。所以提到导数，我们把它当作函数的斜率就好了，正式的斜率定义为在上图这个绿色的小三角形中，高除以宽。
*	斜率=$\frac{df(a)}{da}$ 就表示用一段小到不可量的高df(a) 除以 对应的一段小到不可量的宽da ，就可以得出那一段小到不可量的可看成是一个点的地方的斜率是多少。

* 更多的导数例子
 ![da97af06.png](:storage\d6750442-a6af-46c6-8f74-ac932b78e8b9\da97af06.png)
 	* f(a)=$a^2$ ，如果 a=2 的话，那么 f(a)=4。让我们稍稍往右推进一点点，现在a = 2.001，则 f (a) ≈ 4.004，就是说，a右移0.001，f(a)上移0.004，是右移的4倍，根据规定，$a^2$求导结果是2a,而在a=2这个点上，$a^2$求导结果刚好就等于4，说明这个函数的右移上移比（即斜率或叫导数）并不固定，随a点位置变动而改变。
 	* 其他函数也可以用这种方式理解，即函数在该点的右移一小段与上移一小段的比值，必然等于该函数的导数代入该点后得出的值，这也是f(a)的导数等于$\frac{df(a)}{da}$的解释

* 总结
	* 导数就是斜率，而函数的斜率，在不同的点是不同的。除了一元一次方程例如f(a)=2a。
	* 如果你想知道一个函数的导数，可参考微积分课本或者维基百科，然后应该就能找到这些函数的导数公式。

<br>

### 2.7 计算图（Computation Graph）
* 前向传播
 可以说，一个神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对
应的梯度或导数。
* 例子
	* J(a,b,c)= 3(a+b\*c) 的计算步骤,
		1. u=b\*c
		2. v=a+u
		3. J=3\*v
	* 假设a=5,b=3,c=2,那么J(a,b,c)=33,通过一个从左向右的过程，你可以计算出J的值。为了计算导数，从右到左（红色箭头，和蓝色箭头的过程相反）的过程是用于计算导数最自然的方式。
	![2453b88e.png](:storage\d6750442-a6af-46c6-8f74-ac932b78e8b9\2453b88e.png)

<br>

### 2.8 计算图导数（Derivatives with a Computation Graph）
* J(a,b,c)的偏导数，一个多变量的函数的偏导数，就是它关于其中一个变量的导数而保持其他变量恒定（相对于全导数，在其中所有变量都允许变化）。求导过程就把其他变量看成常数，只当目标变量是变量来求导就可以了。
* 用反向传播概念解释求导
	![601246f4.png](:storage\d6750442-a6af-46c6-8f74-ac932b78e8b9\601246f4.png)
	图中红色箭头表达求导过程同时也是反向传播过程，
	
* 用之前解释函数求导原理的方法解释多变量函数求导就是：
	* 当对J(a,b,c)函数中的a变量求导时，我们需要知道的是当a增加一小段时，J(a,b,c)增加了多少，从而计算出a和J(a,b,c)增加的比值，这个过程b,c应该保持不变不应该影响J的增加
* 链式法则
	J(a,b,c)= 3(a+b\*c) 
	设：a=5　,　b=3　,　c=2  ,
	　　u=b\*c　,　v=a+u　,　J=3\*v
	* 假如变量a变化，那么v就会被改变，通过改变v就会改变J。
	* 而根据之前解释函数求导原理的方法可知，a变化对v的影响，是由$\frac{dv}{da}$决定，同样地，v对J的影响是由$\frac{dJ}{dv}$决定。
	* 两个斜率与$\frac{dJ}{da}$的关系，由于a和其他参数运算的结果是v，例如a增0.001；v增0.001说明a影响v是1倍关系，v和其他参数运算结果是J，v增0.001；J增0.003说明v影响J是3倍关系，a变化1倍影响的v变化会再3倍影响J，所以两个斜率应该相乘才能体现da对dJ的影响。
	* 从数理上$\frac{dv}{da}$\*$\frac{dJ}{dv}$,前一个参数的分子与后一参数分母相约也刚好等于$\frac{dJ}{da}$，由此可知$\frac{dJ}{da}$=$\frac{dv}{da}$\*$\frac{dJ}{dv}$

* 符号约定
	* J函数在流程图里是最终输出变量(正向传播)，命名为FinalOutputVar，而各种变量命名为Var，所以J函数对各个变量的求导就写为，$\frac{dFinalOutputVar}{dVar}$，而在编程语言中，只需写：d(变量名) 就代表这个变量对函数J的求导，如数学上：$\frac{dJ}{da}$ 在编程里写成：d(a)
	
	



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTY3MTgxNTI2Ml19
-->