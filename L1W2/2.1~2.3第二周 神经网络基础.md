
# 2.1~2.3第二周 神经网络基础


### 常用变量名称释义
* x	输入变量
* y	输出结果
* 特征向量x	
为了把这些像素值放到一个特征向量中，我们需要把这些像素值提取出来，然后放入一个特征向量x，直到得到一个特征向量，把图片中所有的红、绿、蓝像素值都列出来
<br>
* n<sub>x</sub> 表示特征向量x的维度
如64\*64大小的彩色图片，每个像素都有
RGB三个值，所以总维度n<sub>x</sub>是64\*64\*3=12288
<br>
* 符号定义 ：
X ：表示一个 n<sub>x</sub> 维数据，为输入数据，维度为( n<sub>x</sub> , 1 )
Y ：表示输出结果，取值为 ( 1, 0 )；
( x<sup>(i)</sup> , y<sup>(i)</sup>)：表示第i 组数据，可能是训练数据，也可能是测试数据，此处默认为训练
数据，即第i张图片；
X=[x<sup>(1)</sup>,x<sup>(2)</sup>,x<sup>(3)</sup>,.....x<sup>(m)</sup>]：表示所有的训练数据集的输入值，放在一个 n<sub>x</sub>\* m 的矩阵中，一个 n<sub>x</sub>表示一张图片，m 表示样本数目，即图片数量;
Y=[y<sup>(1)</sup>,y<sup>(2)</sup>,y<sup>(3)</sup>,.....y<sup>(m)</sup>]：对应表示所有训练数据集的输出值，维度为 1\*m。
	用一对(x,y) 来表示一个单独的样本，x代表 n<sub>x</sub> 维的特征向量， y 表示标签(输出结果)
只能为0或1。
补充：为了强调这是训练样本的个数，会写作M<sub>train</sub> ，当涉及到测试集的时候，我们会使用M<sub>text</sub>来表示测试集的样本数.

* 总结
X 与先前的表示单张图片的X再与m相乘的大矩阵，是一个规模为n<sub>x</sub>乘以m 的矩阵，纵轴表示一张图片的所有RGB数值，横轴是图片数量m，当你用Python 实现的时候x.shape = (n<sub>x</sub> ,m)
Y 在这里是一个规模为1 乘以m 的矩阵，每一个X对应一个输出结果再与图片数量m相乘，同样地使用Python 将表示为 y.shape = (1 ,m)
<br>


### 逻辑回归模型
* 线性回归函数： $\hat{y}$ = w<sup>T</sup>x + b
	$\hat{y}$表示输出y 等于1 （即图片是猫）的可能性或者是概率
	X 表示输入集合，一个n<sub>x</sub>维的向量，包含所有输入图片$x_1,x_2$..
	w 表示逻辑回归的参数,也是一个$n_x$维向量(和X维度一样)，w<sup>T</sup>表示w的转置，b是一个实数（表示偏差）
	**补充：为什么是w的转置与X相乘**，因为w与X有相同维度$n_x$，即相同行数，矩阵乘法里第一个矩阵的列数与第二个矩阵的行数相同矩阵相乘才有意义，所以w原来与x行数相同，转置后就是列数与x行数相同，才能相乘
<br>

* 线性函数转换为非线性sigmoid函数
	* 让$\hat{y}$表示实际值y等于1 的机率的话， $\hat{y}$ 应该在0 到1 之间，但上面的线性函数输出范围不一定在0到1之间，因此在逻辑回归中，我们的输出应该是$\hat{y}$ 等于由上面得到的线性函数式子作为自变量的sigmoid 函数中
	* sigmoid 函数公式:  
	$$\frac{1}{1+e^{-z}}$$
	其中z = w<sup>T</sup>x + b

	<center>  <img src="http:\\omw27y2pe.bkt.clouddn.com\image\sigmoid\Logistic-curve.png" width="50%" height="50%" \></center>
	<center> sigmoid 函数图像 </center>
* 目标
你的工作就是去让机器学习参数w以及b 这样才使得$\hat{y}$ 成为对y = 1这一情况的概率的一个很好的估计。


### 逻辑回归模型的成本函数
* 逻辑回归的输出函数：
	* $\hat{y} =\sigma(w^Tx + b)$,where $\sigma$(z)=$\frac{1}{1+e^{-z}}$,其中 $z = w^Tx + b$
<br>
	* Given{X=[(x<sup>(1)</sup>,y<sup>(1)</sup>),(x<sup>(2)</sup>,y<sup>(2)</sup>),..,(x<sup>(m)</sup>,y<sup>(m)</sup>)]},want $\hat{y}$<sup>(i)</sup> 约等于 $y^{(i)}$ 
<br>
	* 说明：即$y^{(i)}$是根据X对应的正确结果，是已知的，$\hat{y}^{(i)}$是逻辑回归模型分析出的结果，我们就是要不断修正模型的w和b值，让分析结果越接近正确结果。

* 损失函数（成本函数）
	* Loss function: L($\hat{y}$，y)
	我们通过这个L 称为的损失函数，来衡量预测输出值和实际值有多接近。
	* 我们在逻辑回归中用到的损失函数是：
L($\hat{y}$，y) = -(ylog$\hat{y}$ +(1-y)log(1 -$\hat{y}$)) 
		* 当 y=1 时损失函数L = -log$\hat{y}$ 如果想要损失函数L尽可能小，那么$\hat{y}$ 就要尽可能大，因为sigmoid 函数取值[0,1],所以$\hat{y}$会无限接近于1。
		* 当 y = 0时损失函数L = -log(1-$\hat{y}$) ,如果想要损失函数L尽可能小，那么 $\hat{y}$就要尽可能小，因为sigmoid 函数取值[0,1],所以$\hat{y}$会无限接近于0。
	* 总结：在这门课中有很多的函数效果和现在这个类似，就是如果y 等于1，我们就尽可能让$\hat{y}$变大，如果y 等于0，我们就尽可能让 $\hat{y}$ 变小。



* 成本函数
	损失函数是衡量算法对单个样本的表现，我们还需要定义一个对整个算法的代价函数，定义是对m个样本的损失函数求和然后除以m:
	$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^i)=-\frac{1}{m}\sum_{i=1}^m(ylog\hat{y} +(1-y)log(1 -\hat{y}))$$

* 总结
损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的 w 和 b ，来让成本函数 J 的总代价降到最低。



<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg3ODY1MTAxNiwtMTk2ODY3MzQwXX0=
-->
