
# 2.15~2.18第二周 神经网络基础
[TOC]

### 2.15 Python 中的广播机制（Broadcasting in Python）
![ea243103.png](:storage\964c5925-1e39-4450-8124-7afe77a826b0\ea243103.png)
&emsp; 用矩阵运算的方式计算不同食物中不同营养成分中的卡路里百分比。
* 求和矩阵cal=目标矩阵A.sum(axis=0)
	&emsp; 上式表示对矩阵A按列求和，axis 用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1 轴是水平的，也就是行。

* 和矩阵cal.reshape(1,4)
&emsp; 将矩阵cal重塑成1\*4矩阵，当我们写代码时不确定矩阵维度的时候，通常会对矩阵
进行重塑来确保得到我们想要的是列向量或行向量。
&emsp; 另外，重塑操作reshape是一个常量时间的操作，时间复杂度是O(1)，它的调用代价极低。

* numpy的广播机制
	* &emsp; 当一个1\*4的列向量与一个常数做加法时，实际上会将常数扩展为一个1\*4的列向量，然后两者做逐元素加法。 **注意：** 这种广播机制只对单行向量和单列向量可以使用。
	* &emsp; 泛化形式是n\*m的矩阵和n\*1或1\*m的矩阵相加。在执行加法操作时，其实是将n\*1或1\*m矩阵复制成为n\*m的矩阵，然后两者做逐元素加法得到结果。
<br>

* 广播机制的通用形式
	* &emsp;  一个m\*n矩阵 加/减/乘/除 一个 n\*1或1\*m的单行或列矩阵，这个行或列矩阵n\*1或1\*m矩阵都会复制自身成为m\*n矩阵，然后才与那个m\*n矩阵逐元素 加/减/乘/除
	* &emsp;  此通用形式不仅对单行或列矩阵有效，对实数也有同样效果，实数与m\*n矩阵四则运算前也会自身先扩展成一个m\*n矩阵再与这个矩阵逐元素运算。
	* &emsp;  单行或列矩阵与实数四则运算，实数也执行广播机制再逐元素运算,同样地，两个单行和单列矩阵之间的运算也适用。

<br>

### 2.16 关于 Python 与numpy 向量的使用（A note on python or numpy vectors）
* python广播的优缺点
	* 优点：灵活，一行代码就能完成很多功能。
	* 缺点：必须熟悉广播机制，否则容易产生意外的结果，但程序又不会报错。

* python-numpy的一些使用细节
	* &emsp;  a=np.random.randn(5)，这样会生成存储在数组a 中的5 个高斯随机数变量。打印a时，a的形式跟一个行向量格式一样，但在python里，这是一个一维数组，不是行向量。
	* &emsp;  **注意：** 向量和数组的区别，数组是编程概念，一种数据结构，与单向量格式非常相似，用作暂时保存一系列数据，但对数组进行科学计算时，结果会和向量的不一样，所以在逻辑回归和神经网络需要矩阵运算时，必须确保是用行/列向量计算而不是数组。
	<br>
	* 如何区分向量和数组
		* &emsp; 刚刚代码a=np.random.randn(5)生成的数组，其数据结构(a.shape)是(5,);而行向量的数据结构(行向量.shape)应该是(1,5);列向量的数据结构(行向量.shape)应该是(5,1)
		* 一个细微的差别，在这种数据结构中，当我们输出a的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。
	 &emsp;一维数组：[ 1.  1.  1. -1.  0.]
	 &emsp;单行矩阵：\[[-1. -1. -1.  2. -0.]]


* 断言语句(assertion statement)
	&emsp;用python的断言语法，检查数据结构是数组还是矩阵：
	$$assert(a.shape==(5,1))$$
	
* &emsp;另外，为了确保你的矩阵或向量所需要的维数时，不要羞于重塑矩阵reshape操作。	
	$$cal=A.reshape(5,1)$$
<br>

### 2.18 逻辑回归损失函数详解（Explanation of logistic regression cost function）
* 回归逻辑中，预测结果表示为：
	* $\hat{y} =\sigma(w^Tx + b)$
	* 其中$\sigma$是S型函数，$\sigma$(z)=$\frac{1}{1+e^{-z}}$
	* &emsp;  我们规定 $\hat{y}=p(y=1|x)$,意思是给定训练样本x，得出预测结果是y=1的概率。反过来说，给定训练样本x得出预测结果是y=0的概率就是1-$\hat{y}$,得出：
	$$if(y==1): p(y|x)=\hat{y}$$	
	$$if(y==0): p(y|x)=1-\hat{y}$$	

* 将上述公式表达为一条公式
	&emsp;  需要注意的是，我们现在讨论的是**二分类问题**的损失函数，y只能取值1或者0，所以上述条件概率公式可以表达为：
	$$p(y|x)=\hat{y}^y*(1-\hat{y})^{1-y}$$
  	&emsp;  当y=1时，代入可知 $p(1|x)=\hat{y}$
	&emsp;  当y=0时，代入可知 $p(0|x)=1-\hat{y}$

<br>

* **似然函数和最大似然估计(MLE)**
	&emsp;  概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果。P(A|B)表示在B条件下A的概率，P(B)为事件B的概率。
	&emsp;  利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。L(θ|y)表示给定输出y时，关于参数θ的似然函数，在数值上他等于P(Y=y|θ)，表示参数θ下Y=y的概率。
	&emsp;  在我们的逻辑回归中，在给定输出y时，关于参数θ,x的似然函数为p(y|x,θ),而我们就希望把这个概率最大化，最极端理想的情况就是根据输入x和θ，输出y为约定值的概率必然等于1。
	&emsp;  设$\hat{y}$是根据x和θ推定输出y=1的概率,推出y=0的概率是1-$\hat{y}$，这样实际结果是y=1时，$\hat{y}$应该增大，实际结果是y=0时，$\hat{y}$应该减少。
	*  $p_1=p(y=1|x,θ)=\hat{y},y=1$
	*  $p_0=p(y=0|x,θ)=1-\hat{y},y=0$
	* 融合成一条公式的似然函数：
		* $p=p(y|x,θ)=\hat{y}^y*(1-\hat{y})^{1-y}$
	

* **对数化似然函数**
	&emsp;  θ的极大似然估计就归结为求似然函数的最大值点。由于似然函数是单调增函数，Log函数也是单调递增函数，取对数可以方便计算，因为在极大似然估计中，直接求导比较困难，所以通常都是先取对数再求导找极值点。
$$log(p)=log(p(y|x,θ))=log(\hat{y}^y*(1-\hat{y})^{1-y})$$
$$log(p)=ylog\hat{y} +(1-y)log(1 -\hat{y})$$
<br>

* **似然函数转向求损失函数**
	&emsp;  在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。
	&emsp;  而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —-> min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。
	$$-log(p)=-(ylog\hat{y} +(1-y)log(1 -\hat{y}))$$
<br>

* **m个训练集的情况**
	* 出发点：
	&emsp;  分析整个训练集中的样本值(label 要么是0，要么是1) 的概率，假设所有的训练样本服从同一分布且相互独立，也即独立同分布的所有这些样本的联合概率就是每个样本概率的乘积。
	&emsp;  上面提到最大似然函数数值上等于p(y|x,θ)，我们需要寻找一个参数θ最大化似然函数数值，现在从1到m的每个训练样本x下得出y的概率累乘积同样适用最大似然估计，寻找这个累乘积的最大值就能找到最适合m个样本最大概率推出m个正确y的参数θ。
<br>
	* m个样本的似然函数累乘积:
	 	* $\prod^m_{i=1}p(y^{(i)}|x^{(i)},θ)$
	* &emsp;  同样地累乘公式不方便求导，先把它log化，同样是单调递增函数，并不影响求最大值,然后根据log运算法则，累积就变成累加了:
		* $log(\prod^m_{i=1}p(y^{(i)}|x^{(i)},θ)$
$=\sum^m_{i=1}logp(y^{(i)}|x^{(i)},θ)$
<br>	
 	* 最大似然转求最小损失
&emsp;  由上面单个样本的似然函数我们知道，求最大化训练样本集出现的概率，从另外一个角度看相当于求最小化损失函数。所以把单个最大似然估计公式转换成最小损失函数的负值就得出：
		因为：
		$$p(y^{(i)}|x^{(i)},θ)=-L(\hat{y}^i,y^i)$$
		所以：
		$$\sum^m_{i=1}logp(y^{(i)}|x^{(i)},θ)=-\sum^m_{i=1}L(\hat{y}^i,y^i)$$
	* &emsp;  而m个单独样本的损失函数累加就是整个m样本的成本函数J:
		$$J(w,b)=\sum^m_{i=1}L(\hat{y}^i,y^i)=-\sum^m_{i=1}logp(y^{(i)}|x^{(i)},θ)$$
	* &emsp;  最终为了化简成本函数，可以对成本函数进行适当的缩放，在前面乘以1/m的一个常数因子:
	$$J(w,b)=-\frac{1}{m}\sum^m_{i=1}logp(y^{(i)}|x^{(i)},θ)$$

* **总结**
	&emsp;  整个推演思路就是，为了求最小化成本函数J(w,b)我们从逻辑回归模型的最大似然估计的角度出发在训练集中的样本都是独立同分布的条件下，将(将最大似然估计与最小损失函数等价起来)。


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEwMjQ3MTI3NTJdfQ==
-->
