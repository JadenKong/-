# 2.20第二周 神经网络基础

### 作业2_2出现的知识点2 算法类知识

* **一般对新数据集的预处理的步骤**
	1. 数据矩阵的维度形状，理清数据的样本数量，单个样本尺寸等信息
	2. 将数据矩阵重塑成1维矩阵，例如reshape(hight * width * 3, 1)

* **学习算法的总体结构**

<center><img src=D:/Python/Kaggle/神经网络和深度学习/第一课第二周编程作业/assignment2/images/LogReg_kiank.png></center>

<br>

* **初始化参数**
	* $$z^{(i)} = w^T x^{(i)} + b$$
	其中w是一个全0矩阵，b是常量0语法：
	w=np.zeros(1维长度，2维长度，..) b=0

<br>

* **合并所有函数到运算模型中**
	* Initialize (w,b)初始化参数w,b
		* w是一个列数与X一张图片的所有元素个数相等的全0行向量
		* b是一个常数0
	* 构建sigmoid函数
		* 根据公式：
			$$sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$$
		* 代码：s = 1 / (1 + np.exp(-z))
	* propagate()函数利用公式的前向传播计算出dw,db和cost函数值
		* 计算出A值：A = sigmoid(np.dot(w.T, X) + b)  
		* 根据公式计算出cost损失函数值：
			* 公式：
				$$J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$$
			* 代码：cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))
		* 根据公式计算出w对损失函数求偏导dw：
			* 公式：
				$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$$
			* 代码： dw = 1 / m * np.dot(X, (A - Y).T)
		* 根据公式计算出b对损失函数求偏导db:
			* 公式：
				$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$$
			* 代码：db = 1 / m * np.sum(A - Y)
	* 构建梯度下降函数optimize()
		* 从上一个函数propagate()获得dw,db,cost函数值
		* 根据梯度下降公式，更新w,b值
			* 公式：
				$$ \theta = \theta - \alpha \text{ } d\theta$$
			* 代码：    
				w = w - learning_rate * dw
				b = b - learning_rate * db
		* 反复循环上述操作，一般循环100次，最后输出w,b，dw,db，cost函数值
	* 预测函数predict()
		* 保证w矩阵的长度是X集合一样的行数，代码： w = w.reshape(X.shape[0], 1)
		* 预测结果是一个行向量，长度与X样本数量一样，Y_prediction = np.zeros((1,X.shape[1]))
		* 根据前面梯度下降optimize()之后得出的w,b计算预测值A，代码：A = sigmoid(np.dot(w.T, X) + b)
		* 对A值进行分类，大于0.5归类为1，小于0.5归类为0


* 求平均值numpy.mean(矩阵a, axis, dtype, out，keepdims )

<br>

* **过度拟合overfitting**
	&emsp; 为了提高训练样本的准确率，增加变量number of iterations梯度下降循环次数，会发现出来的结果w,b对训练样本的准确率提高了，但可能对测试样本的准确率下降了，这就是过拟合。
	
<br>

* **学习率$\alpha$的选择**
	* 三种不同的学习率比较
![](D:/Python/Kaggle/神经网络和深度学习/第一课第二周编程作业/assignment2/images/learningrate比较.png "")
	* 结论：
		* 如果学习率设置太大，损失函数曲线可能会上下波动，甚至发散。
		* 学习率不是越小越好，要防止过拟合，如果训练准确率远高于测试准确率，就是过拟合了。
		* 选择可以让损失函数值降更低的学习率，如果出现过拟合情况，就要用其他算法模型优化

<br>

* **scipy.ndimage.imread**(fname,flatten=False): 将图片转换为数组，如果mode默认为‘RGB’，则返回的数组的形状为（height , width, 3)

* 调整图像大小：**scipy.misc.imresize**(arr:要调整的数组，size:调整大小的数值，interp : str, optional(用于重新调整图像插值，不常用)，mode:模式，默认2D图像是L模式，3D图像是RGB模式)
* 

* **作业总结：**
	* 预处理数据集很重要
	* 把整个功能分开几个函数实现： initialize(), propagate(), optimize()。然后用model()函数整合以上函数。
	* 调整学习率将会对算法结果产生很大影响。