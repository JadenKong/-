### 2.19第二周 神经网络基础
[TOC]

### 作业2_1出现的新知识

* **范数**
	&emsp; 范数是一个函数，是矢量空间内的所有矢量赋予非零的正长度或大小。顾名思义作用是用来规范向量中的元素的。通过这种规范化，初步筛选掉X矩阵想里面没用的特征(元素)。
	* 常用的三种范数：
		1.一范数,列和的最大值
		&emsp; ║A║1 = $\sum^n_1|x_i|$；
		2.二范数,求特征值，然后求最大特征值得算术平方根
		&emsp; ║A║2 = $\sqrt{\sum^n_1{x_i}^2}$
		3.无穷范数,行和的最大值
		&emsp; ║A║∞ = $max(|x_i|)$
	* python中的范数函数使用np.linalg.norm()
		* 参数说明：np.linalg.norm(输入的矩阵x,范数类型ord,行列处理类型axis,是否保持矩阵的二维特性keepding)
	* 特别说明python numpy函数中axis参数
	&emsp;	当axis为0时,是压缩行,即将每一列的元素相加,将矩阵压缩为一行；当axis为1时,是压缩列,即将每一行的元素相加,将矩阵压缩为一列 
<br>

* **softmax function 归一化函数**	
&emsp;	$\text{for } x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}
    x_1  &&
    x_2 &&
    ...  &&
    x_n  
\end{bmatrix}) = \begin{bmatrix}
     \frac{e^{x_1}}{\sum_{j}e^{x_j}} &&
    \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &&
    ...  &&
    \frac{e^{x_n}}{\sum_{j}e^{x_j}} 
\end{bmatrix}$ 
	* 作用：
	&emsp;	softmax函数把一个k维的real value向量（a1,a2,a3,….）映射成向量（b1,b2,b3,….）其中bi是一个0-1的常数，然后可以根据bi的大小来进行多分类的任务，如取权重最大的一维。 
<br>

* **三种数组相乘的方式比较**
	1.	np.dot()点乘
		* 一维矩阵相乘，计算内积，得到一个值
		* 多维矩阵相乘，只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义
	2. np.outer(参数1，参数2)倍乘
		* 假如参数是多维向量，坍塌重塑为一维向量
		* 第一个参数表示倍数，表示将第二个参数的一维向量形式乘以几倍。
		* 第一个参数确定结果的行，有多少个元素就有第二个参数的几多个倍数后结果；第二个参数确定结果的列，是每一个自身倍数之后的结果
	3. np.multiply(x1,x2)逐元素相乘
		* 对应位置进行加减乘除
		* 两个参数和结果的shape应该一致
<br>

* **L1和L2损失函数**
	* L1损失函数：
	&emsp;	返回遍历矩阵里面每个y(i)-yhat(i)的绝对值之和，最后得出一个值而不是矩阵：
	$$L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| $$
	* L2损失函数：
	&emsp;	求y(i)-yhat(i)的平方可以用y(i)-yhat(i)乘y(i)-yhat(i),这个相乘是点乘，不是逐元素平方，因为损失函数最后得出的是一个值而不是矩阵。所以要乘y(i)-yhat(i)的转置才是平方相乘的效果。同时这样可以不用for循环求每层{y(i)-yhat(i)}*{y(i)-yhat(i)}。
公式：
		$$L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 $$
		
<br>

### 作业2_2出现的新知识1

* numpy的三个函数shape，reshape，squeeze
	* numpy.shape()查看矩阵或数组各维度的长度
	例子：3×3的单位矩阵e, e.shape为（3，3），表示矩阵e的1维长度为3，2维长度为3。
		<br>
	* numpy.reshape()重塑矩阵或数组的维度
		* 例子：
		&emsp;	数组e=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
		&emsp;	e.reshape(1,1,10)
		&emsp;	数组e边矩阵e：array(\[\[\[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])
		*	数组e被重塑成1维长度为1，2维长度为1，3维长度为10的矩阵。
		*	补充：如果reshape(num,-1),-1参数表示不写明该维度具体长度，根据其他维度的长度设置，剩下没设置的长度就分到-1那个维度，譬如矩阵总共有xn个元素，0维度长度是num的话，1维度长度就是xn/num
	<br>
	* numpy.squeeze()从数组的形状中删除单维度条目，即把shape中为1的维度去掉。
		* 例子：对上面的矩阵e：np.squeeze(e)
	&emsp;	矩阵e变成：
	&emsp;	array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
		* 除了原来的长度为10的3维被保留外，其余两个长度为1的维度都被去掉了

	* np.vstack(x1,x2)增加行数长度的合并矩阵
		适合两个维度数shape相同的矩阵运算:
		* a = np.array([[1,2],[3,4],[5,6]])
		* b = np.array([[10,20],[30,40],[50,60]])
		* np.vstack((a,b))
			array([[ 1,  2],
					[ 3,  4],
					[ 5,  6],
					[10, 20],
					[30, 40],
					[50, 60]])
<br>
	* np.hstack((a,b))增加列数长度的合并矩阵
	适合两个维度数shape相同的矩阵运算:
		* np.hstack((a,b))
			array([[ 1,  2, 10, 20],
       		[ 3,  4, 30, 40],
       		[ 5,  6, 50, 60]])
			
			
* 训练数据集合的简介：
从之前的代码可以知道，在train_dataset里面：
{'list_classes': <HDF5 dataset "list_classes": shape (2,), type "|S7">, 
'train_set_x': <HDF5 dataset "train_set_x": shape (209, 64, 64, 3), type "|u1">,
'train_set_y': <HDF5 dataset "train_set_y": shape (209,), type "<i8">}
可以看到train_set_x一维长度209，表示图像个数209；二维长度三维长度均为64表表示图片长宽均是64；四维长度表示该点颜色，长度为3因为RGB有3个数值

<br>