# 4.5-4.8 第四周 神经网络基础
[TOC]

### 4.5 为什么使用深层表示？（Why deep representations?）
* #####例子
	* &emsp; 当你在做人脸识别或是人脸检测系统，你可以把第一层看作一个特征探测器或者边缘探测器（一个小方块就是一个隐藏单元，它会找这个图像边缘的方向），第二层可以看作由找到的边缘进行组合合成脸部的不同部分，第三层就可以识别和探测不同的人脸了，更细致的了解需要去学习卷积神经网络。
	![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\d71855c2.png)
	* &emsp; 语音识别也是如此，先从低层次的音频波形找到一些特征，再寻找音位，组合成单词、词组直到完整的句子，这些从简单到复杂的金字塔型组合同样也是其他一些深度学习的灵感来源。
<br>

* #####神经网络为何如此高效？
	*  &emsp; 它来源于电路原理，它和你能够用电路元件计算哪些函数有着分不开的联系，根据不同的基本逻辑门可以组合成很复杂的电路。
	* &emsp; 深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。
	* ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\18242b12.png)

<br>

### 4.6 搭建神经网络块（Building blocks of deep neural networks）
* &emsp; 神经网络里的一个层相当于一个神经网络的组成元件，用之前学习的前向传播和反向传播原理，了解神经网络中的一个元件怎么运作。

<br>

* ##### 正向函数
	* 输入为a^[l-1]^,输出为a^[l]^,原理：
		* $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$
		* $a^{[l]}=g^{[l]}(z^{[l]})$
	* 运算中需要用到:W^[l]^,b^[l]^,还有计算输出缓存到cache的z^[l]^ 。
<br>

* ##### 反向函数
	* 输入为da^[l]^,输出为da^[l-1]^,原理：
		* $da^{[l-1]}=\frac{dJ}{da^{[l]}}·\frac{da^{[l]}}{da^{[l-1]}}$
	* 运算中需要用到:W^[l]^,b^[l]^,还有需要缓存到cache的dz^[l]^ ,得出dz^[l]^ 就可计算该层的dW^[l]^,db^[l]^。
		* ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\a858b26b.png)

* ##### 多层的前向传播和反向传播过程
	![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\\832f8b07-a3a5-4324-af02-a146fd2fcf73\6003dfcc.png)
    * 注意：
    	* &emsp; 在各层的反向传播计算中要用到的W^[l]^,b^[l]^,z^[l]^参数，可以在计算该层的前向传播时得出并保存到cache，然后到反向传播时直接调用cache获取W^[l]^,b^[l]^,z^[l]^就可以。
<br>

### 4.7 参数 VS超参数（Parameters vs Hyperparameters）

* ##### 参数与超参数
	* &emsp; 参数：神经网络的参数是指 W 和 b 。
	* &emsp; 超参数：比如算法中的 learing rateα（学习率）、 iterations(梯度下降法循环的数量)、L（隐藏层数目）、n[l]（每个隐藏层神经单元数目）、choice of activation function（激活函数的选择）都需要你来设置，这些变量的设置最后实际上控制了参数 W 和 b 的值，所以它们被称作超参数。
	
    * 如何寻找超参数的最优值？
    	* &emsp; 走 Idea—Code—Experiment—Idea 这个循环，尝试各种不同的参数，实现模型并观察是否成功，然后再迭代；
    	* &emsp; 比如一个学习率的最优值可能你觉得等于 0.01 最好，你可以实际试下，训练看看效果如何，然后基于尝试的结果可能你会发现提高到 0.05 比较好，找到那个能加快学习过程且收敛在更低的损失函数值的α。
    	* ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\4a01c85f.png)	
    	
    * 经验规律：
    	* &emsp; 经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。
<br>


### 4.8 深度学习和大脑的关联性（What does this have to do with the brain?）
* ##### 现在的深度学习和大脑学习的关联性不大
	* &emsp; 当你在实现一个神经网络的时候，那些公式是你在做的东西，你会做前向传播、反向传播、梯度下降法，计算损失函数。
	* &emsp; 对这些公式，函数很难用简单的方式表述他们，把这些形容为像大脑一样学习思考，其实是过度简化了我们的大脑具体在做什么，但因为这种表述很简洁，也能让普通人更愿意讨论，更吸引大众眼球，但这个类比是非常不准确的。
	* &emsp; 一个神经网络的逻辑单元可以看成是对一个生物神经元的过度简化，迄今为止连神经科学家都很难解释究竟一个神经元能做什么，它可能是极其复杂的，比logistic 回归的运算要复杂得多。
	* &emsp; 深度学习可以理解为一个很好的工具来通过各种很灵活很复杂的函数，研究到从 x 到 y 的映射，在监督学习中研究从输入到输出的映射。
	* &emsp; 也许在神经网络这一学派早期还可以标榜这个学科是在研究模仿大脑学习，但随着神经网络不断发展，现在的神经网络应用的内在运作方式，已经很难和人类大脑运作方式扯上关系。
<br>
	* 总结图：
		![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\\832f8b07-a3a5-4324-af02-a146fd2fcf73\ee9e175a.png)