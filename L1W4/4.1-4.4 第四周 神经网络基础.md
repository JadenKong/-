# 4.1-4.4 第四周 神经网络基础

[TOC]

### 4.1 深层神经网络（ Deep L-layer neural network）

* 从单层网络到深层网络

	* 逻辑回归结构（单个隐藏层单个神经单元结构）和一个隐藏层的神经网络结构。 

	![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\8b0d3444.png)

   	* 两个隐藏层的神经网络结构和五个隐藏层的圣经网络结构

    ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\8f55537a.png)

   	* 注意，神经网络的层数是这么定义：从左到右，由 0开始定，数神经成时，不数输入层，只算隐藏层和输出层。

<br>



* 深层神经网络符号定义

   * ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\df9495f2.png)

	我们用L表示层数，上图：L=4，输入层的索引为“0”，

  	第一个隐藏层n[1]=5,表示有5个隐藏神经元，同理$n^{[2]}=5，n^{[3]}=3，n^{[4]}= n^{[L]}=1$（输出单元为1）。而输入层,$n^{[0]}=nx=3。$

    * &emsp; 输入特征记作X,同样也是0层的激活函数，所以x=a^[0]^

    * &emsp; 在不同层所拥有的神经元的数目，对于每层l 都用a[l]来记作l 层激活后结果，通过正向传播时，计算出a[l]。

    * &emsp; 通过用激活函数g计算z[l]，激活函数也被索引为层数l，然后我们用w[l]来记作在l 层计算z[l]值的权重。类似的，z[l]里的方程b[l]也一样。

    * &emsp; 最后一层激活函数a^[L]^=$\hat{y}$,也就是等于这个神经网络的输出结果。

   

<br>





### 4.2 前向传播和反向传播（Forward and backward propagation）

* ##### 前向传播

	* 输入是a^[l-1]^,输出是a^[l]^,缓存是z^[l]^,$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$

	* ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\9009bb57.png)

	* 所以前向步骤通用公式可以写成：

		* $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$

		* $a^{[l]}=g^{[l]}(z^{[l]})$

<br>

* ##### 反向传播

	* 输入是da^[l]^,输出是da^[l-1]^,dw^[l]^,db^[l]^

	* ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\832f8b07-a3a5-4324-af02-a146fd2fcf73\75372bd5.png)

	* 反向传播公式推导

		1.  dz^[l]^=da^[l]^*g^[l]^`(z^[l]^)

		2.  dw^[l]^=dz^[l]^ a^[l-1]^

		3.  db^[l]^=dz^[l]^

		4.  da^[l-1]^=w^[l]T^dz^[l]^

		5.  dz^[l]^=w^[l+1]T^dz^[l+1]^*g^[l]`^(z^[l]^)

	* 向量化表达反向传播

		1. dZ^[l]^=dA^[l]^*g^[l]^`(Z^[l]^)

		2. dW^[l]^=$\frac{1}{m}$dZ^[l]^ A^[l-1]T^

		3. db^[l]^=$\frac{1}{m}$dz^[l]^np.sum(dZ^[l]^,axis=1,keepdims=True)

		4. dA^[l-1]^=W^[l]T^dZ^[l]^
<br>

* ##### 前向传播和反向传播的开端
	* 前向传播的开端
		* 前向传播的开端比较容易理解，就是输入样本X，也称为A^[0]^
	* 后向传播的开端
		* 后向传播的开端即da^[l]^，对于LR二分类问题最后的输出函数一般为损失函数：$L(a,y) = -ylog(a) - (1-y)log(1 -a)$，然后对这个函数求a^[l]^的偏导：
		$$L'(a,y) =\frac{dL(a,y)}{da}= -\frac{y}{a}+\frac{1-y}{1-a}$$
        * 但这公式是对单个样本的公式，如果要向量化到全部样本则变为：$L'(a,y) =\frac{1}{m} \sum_{i=1}^m (-\frac{y^{(m)}}{a^{(m)}}+\frac{1-y^{(m)}}{1-a^{(m)}})$
        * 多样本的代码实现是这样： dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
        * 这个公式计算dz不方便，实际上我们不需要展示多样本向量化的dA^[l]^,因为我们要求的是dw和db，只需要知道dz即可。
        * 在单样本下:
        * 因为，dz^[l]^=$\frac{dL}{da}\frac{da}{dz}$ , a=sigmoid(z) ,$\frac{da}{dz}=a(1-a)$，所以： 
        * $\frac{dL}{dz^{[l]}}= \frac{dL}{da^{[l]}}\frac{da^{[l]}}{dz^{[l]}}=(-\frac{y}{a^{[l]}}+\frac{1-y}{1-a^{[l]}})(a^{[l]}(1-a^{[l]}))=a^{[l]}-y$
        * 推广到多样本向量化：$dZ^{[l]}=\frac{dL}{dZ^{[l]}}=A^{[l]}-Y$
        * 可以把这个结果当做是后向传播的开端，求得最右端的dW^[l]^和db^[l]^。
<br>

* ##### 神经网络各层使用的激活函数
    * 第一层和第二层是隐藏层，分别使用不同的ReLu激活函数

    * 第三层是输出层，如果输出是个二分类（0或1），就用sigmoid函数做激活函数
	* ![3ef21623.png](D:\MyData\神经网络与深度学习\attachments\\832f8b07-a3a5-4324-af02-a146fd2fcf73\48987b62.png)



<br>



### 4.4 核对矩阵的维数（ Getting your matrix dimensions right）

* #####各个参数的维数通用公式：

	* z^[l]^的维度是（本层的神经单元数，1），而本层的维数即为本层的神经单元数,所以z^[l]^的维数shape：(n^[l]^ ,1)

	* w^[l]^的维度是（本层的维数，前一层的维数），即w^[l]^的维数shape：(n^[l]^ ,n^[l-1]^)

	* b^[l]^的维度是（本层的维数，1），因为w^[l]^与a^[l-1]^点乘，得出矩阵的形状必然等于z^[l]^,所以b^[l]^的维度必须与z^[l]^保持一致，才能和w^[l]^与a^[l-1]^的点乘结果矩阵相加。即b^[l]^的维数shape：(n^[l]^ , 1)

	* 后向传播中，dw[l]和 w[l]维度相同， db[l]和 b[l]维度相同

<br>



* #####向量化后的各参数维数通用公式(把m个样本代入公式里)：

	* Z^[l]^可以看成由每一个单独的Z^[l]^叠加而得到，Z^[l]^=(z^[l][1]^,z^[l][2]^,...,z^[l][m]^),m为训练集大小，所以向量化的Z^[l]^维度不再是(n^[l]^,1),而是(n^[l]^,m)

	* 同理，A^[l]^:(n^[l]^,m),例子：A^[0]^=X=(n^[0]^,m)

	* w和 b向量化维度不变，但由于A^[l-1]^维度变成(n^[l-1]^,m)，w·A^[l-1]^的维度也变成(n^[l]^,m)，b与wX相加时，由于广播机制，b的维度也会自动扩展成(n^[0]^,m),与m个样本逐个相加。

	* 向量化 z,a以及 x的维度会发生变化，因为向量化后每个不同的样本都是经过同样参数w,b运算，所以w,b矩阵不变。但z,a,x会因为多样本，从原来的单个样本的多特征变成，多样本的多特征，所以矩阵形状都变成(n^[l]^,m)

	* 后向传播中，dZ[l]和 Z[l]维度相同， dA[l]和 A[l]维度相同。





<!--stackedit_data:
eyJoaXN0b3J5IjpbMTYyMjk4NzAzXX0=
-->