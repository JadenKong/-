# 4.9 第四周 神经网络基础

[TOC] 

### 4.9 作业中出现的知识点

* ##### np.array(dA, copy=True)
	* 	copy=True 返回dA矩阵的复制，相当于np.copy()方法


* ##### relu_backward(dA, cache)relu函数的求导:
	*   因为ReLu函数，Z>0时，dA/dZ=g`(Z)=1 所以dZ=dA
<br>

* ##### 前向传播保存的元组cache
	* cache 以元组格式保存了两个元组linear_cache, activation_cache
    	* cache = (linear_cache, activation_cache)
    	* linear_cache里面保存了三个矩阵=(A,W,b) activation_cache保存了一个矩阵=(z)

* ##### L-Model Backward 后向传播总函数
	* 启动后向传播前，你需要计算第一个dA[L] 也就是损失函数对最右一层网络A[L]求偏导

	* 反向迭代器reversed(seq)
		* seq -- 要转换的序列，可以是 tuple, string, list 或 range
		* reversed(range(100))表示从100开始倒数