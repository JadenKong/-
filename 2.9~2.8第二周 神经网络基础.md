# 2.9~2.8第二周 神经网络基础
[TOC]

### 2.9 单个样本的逻辑回归的梯度下降（ Logistic Regression Gradient Descent）
　　
&emsp;　本节我们讨论怎样通过计算偏导数来实现逻辑回归的梯度下降算法。它的关键点是几个重要公式，其作用是用来实现逻辑回归中梯度下降算法。

* 运算中需要用到的公式，前向计算代价函数，假设这个样本只有两个特征值$x_1,x_2$。
* ***注意*** ：样本索引号是用括号上标，如样本$x^{(1)}$,$x^{(2)}$，而单个样本里面的某个特征值，如样本$x^{(1)}$图片矩阵里某个数值就用下标表示如$x_1,x_2$
	* 逻辑回归输出模型：
		 $\hat{y} = w^Tx + b$
		 
	* 线性函数转换为非线性sigmoid函数输出模型:
		$\hat{y} = \sigma(z)=\frac{1}{1+e^{-z}}$
		$z =  w_1x_1+w_2x_2+b$
	* 损失函数：
		$L(\hat{y}^{(i)},y^{(i)}) = -y^{(i)}log\hat{y}^{(i)} - (1-y^{(i)})log(1 -\hat{y}^{(i)})$ 
	* 总代价函数：
		$$J(w,b) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^i)=-\frac{1}{m}\sum_{i=1}^m(ylog\hat{y} +(1-y)log(1 -\hat{y}))$$
	* 假设现在只考虑单个样本的情况，即样本总数只有一个的总代价函数定义如下：
	$L(a,y)=-(ylog(a)+(1-a)log(1-a))$
	其中a是逻辑回归的输出即$\hat{y}$，y是样本的标签值。
	* 梯度下降算法：
	$w:=w-\alpha\frac{∂J(w,b)}{∂w}$
	$b:=b-\alpha\frac{∂J(w,b)}{∂b}$
	* 为了使得逻辑回归中最小化代价函数L(a,y)，我们需要做的仅仅是修改参数w和b 的值。
	<br>
* Logistic regression recap(逻辑回归概述)
![ba458875.png](:storage\75bf1d0b-496f-4654-a43b-4df861a8f65a\ba458875.png)
&emsp;　前面我们已经讲解了如何在单个训练样本上计算代价函数的前向步骤。现在让我们来讨论通过反向计算出导数。
	* 前向步骤：
		1. $z = w^Tx + b = w_1x_1+w_2x_2+b$(假设只有x1x2两个特征样本)
		2. $\hat{y} = a=\sigma(z)=\frac{1}{1+e^{-z}}$
		3. $L(a,y) = -ylog(a) - (1-y)log(1 -a)$
	* 因为我们想要计算出的代价函数L(a,y)的导数，首先我们反向计算出步骤3的代价函数L(a,y)关于a的导数，在编写代码时，你只需要用da来表示$\frac{dL(a,y)}{da}$
	对$L(a,y) = -ylog(a) - (1-y)log(1 -a)$ 求a的偏导：
	$$L'(a,y) =\frac{dL(a,y)}{da}= -\frac{y}{a}+\frac{1-y}{1-a}$$
	* 现在再反向一步，根据步骤2算出z对函数L的求导$\frac{dL}{dz}$
		* 因为$\frac{dL}{dz}=\frac{dL}{da}\frac{da}{dz},\frac{da}{dz}=a(1-a)$，***注意***：这里对a函数的z求导结果，用a来表示，方便与前面dL/da的结果计算。
		* 所以
		$$\frac{dL}{dz}= \frac{dL}{da}\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})(a(1-a))=a-y$$
	* 现在进行最后一步反向推导，根据步骤1计算w,b对函数L的求导(影响)，然后代入到梯度下降算法中，其中我们把x1，x2当作梯度下降中的$\alpha$值(步长)
		* 先求出$\frac{∂L}{∂w_1}$，$\frac{∂L}{∂w_2}$和$\frac{∂L}{∂b}$:
			$\frac{dz}{dw_1}=x_1$,　$\frac{dz}{dw_2}=x_2$,　$\frac{dz}{db}=1$
			$dw_1=\frac{∂L}{∂w_1}=\frac{dL}{dz}\frac{dz}{dw_1}=(a-y)x_1$
			$dw_2=\frac{∂L}{∂w_2}=\frac{dL}{dz}\frac{dz}{dw_2}=(a-y)x_2$
			$db=\frac{∂L}{∂b}=\frac{dL}{dz}\frac{dz}{db}=a-y$
		* 然后代入梯度下降算法：
			更新$w_1:=w_1-\alpha *dw_1=w_1-\alpha(a-y)x_1$
			更新$w_2:=w_2-\alpha *dw_2=w_2-\alpha(a-y)x_2$
			更新$b:=b-\alpha *db=b-\alpha(a-y)$
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIwOTI0ODk2NDFdfQ==
-->